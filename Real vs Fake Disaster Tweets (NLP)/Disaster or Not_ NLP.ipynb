{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Using NLP to Determine Real vs Fake Disasters\n\nThis Kaggle competetion involves using NLP to analayze a set of tweets regaurding disasters, and determine whether or not they reffer to real or fake disasters.\n\nFor the most part, I will be following the information and methods contained in these articles and videos: \n* https://towardsdatascience.com/natural-language-processing-nlp-for-machine-learning-d44498845d5b\n* https://www.youtube.com/watch?v=UvsQPsrZTK4\n* https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport nltk\nimport string\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n!pip install pyspellchecker \nfrom spellchecker import SpellChecker\nfrom operator import truediv\n","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting pyspellchecker\n  Downloading pyspellchecker-0.5.4-py2.py3-none-any.whl (1.9 MB)\n\u001b[K     |████████████████████████████████| 1.9 MB 2.7 MB/s eta 0:00:01\n\u001b[?25hInstalling collected packages: pyspellchecker\nSuccessfully installed pyspellchecker-0.5.4\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Importing the Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"test_url = \"https://raw.githubusercontent.com/davidblumenstiel/Kaggle/master/Real%20vs%20Fake%20Disaster%20Tweets%20(NLP)/test.csv\"\ntrain_url = \"https://raw.githubusercontent.com/davidblumenstiel/Kaggle/master/Real%20vs%20Fake%20Disaster%20Tweets%20(NLP)/train.csv\"\n    \ntest = pd.read_csv(test_url)\ntrain = pd.read_csv(train_url)","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Exploratory Data Analysis\nfirst off, let's get an idea of what the datasets we're given look like"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train.shape)\nprint(train.columns)\nprint(test.shape)\nprint(test.columns)\nprint(train.head)\nprint(train.describe())  #about 43% of the tweets were about real disasters:","execution_count":3,"outputs":[{"output_type":"stream","text":"(7613, 5)\nIndex(['id', 'keyword', 'location', 'text', 'target'], dtype='object')\n(3263, 4)\nIndex(['id', 'keyword', 'location', 'text'], dtype='object')\n<bound method NDFrame.head of          id keyword location  \\\n0         1     NaN      NaN   \n1         4     NaN      NaN   \n2         5     NaN      NaN   \n3         6     NaN      NaN   \n4         7     NaN      NaN   \n...     ...     ...      ...   \n7608  10869     NaN      NaN   \n7609  10870     NaN      NaN   \n7610  10871     NaN      NaN   \n7611  10872     NaN      NaN   \n7612  10873     NaN      NaN   \n\n                                                   text  target  \n0     Our Deeds are the Reason of this #earthquake M...       1  \n1                Forest fire near La Ronge Sask. Canada       1  \n2     All residents asked to 'shelter in place' are ...       1  \n3     13,000 people receive #wildfires evacuation or...       1  \n4     Just got sent this photo from Ruby #Alaska as ...       1  \n...                                                 ...     ...  \n7608  Two giant cranes holding a bridge collapse int...       1  \n7609  @aria_ahrary @TheTawniest The out of control w...       1  \n7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...       1  \n7611  Police investigating after an e-bike collided ...       1  \n7612  The Latest: More Homes Razed by Northern Calif...       1  \n\n[7613 rows x 5 columns]>\n                 id      target\ncount   7613.000000  7613.00000\nmean    5441.934848     0.42966\nstd     3137.116090     0.49506\nmin        1.000000     0.00000\n25%     2734.000000     0.00000\n50%     5408.000000     0.00000\n75%     8146.000000     1.00000\nmax    10873.000000     1.00000\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"## Joining the Datasets\nWe'll need the process the data (get it ready for NLP) as one dataset.  Here, the datasets are joined and indexed by ID.  Lables are also created for later differentiation between testing and training data; outcomes from the training set are also set aside in their own list."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adds lables to each set so we can seperate them again later\ntrain['label'] = 'train'\ntest['label'] = 'test'\n\n#Makes a list of outcomes for the training set\ntrain_target = train['target']\n\n#Combines the datasets\ndf = pd.concat([train,test])\n\n#Transforms all the column names to uppercase to differentiate them from the terms which will be added later\ndf.columns = [x.upper() for x in df.columns]\n\n#Sets the index to the ID column\ndf = df.set_index('ID')\nprint(df.head())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation\nBy processing the data, we can cut down on the noise and get better results from our model.  Here, we will:\n* Make every character lowercase\n* Remove the punctuation\n* Split up tweets into seperate words (for processing)\n* Remove words that don't tell us much (stopwords)\n* Lemmatize the words; group's the same words and variants thereof, or with different inflections, to the same term\n* Join the tweets back together again"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Transforms all chars to lowercase and stores the text sentances in a list\nlower = [x.lower() for x in df['TEXT']]\n\n#Removes punctuation\nnopunct = []  #Going to do each cleaning process in a seperate list\nfor text in lower:\n    nopunct.append(\"\".join([x for x in text if x not in string.punctuation]))\n    \n    \n#Splits each tweet into seperate words\nseperate = []    \nfor text in nopunct:\n    seperate.append(re.split('\\W+',text))\n    \n\n#Removes Stopwords\nnostop = []\nstopwords = nltk.corpus.stopwords.words('english')\nfor text in seperate:\n    nostop.append([x for x in text if x not in stopwords])\n    \n\n#Lemmatizes the words.  Should be more useful than stemming\nlemmatizer = nltk.WordNetLemmatizer()\nlemmat = []\nfor text in nostop:\n    lemmat.append([lemmatizer.lemmatize(x) for x in text])\n    \n\n#Joins the sentances back together\ndocs = []\nfor strs in lemmat:\n    docs.append(' '.join(strs))\n    \nprint(docs[0:4])\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## What Differentiates Disaster Tweets from Non-Disasters\nHere' we'll examine the training dataset and see if we can get any insight into what makes an actual disaster tweet.\nI expect real disaster tweets to come primarily from news organizations, which likely do a better job spell checking their tweets.  Let's see if that's true, along with some other metrics."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Here, we split the training set out of the combined data (before we rejoined the sentances), and into the real and fake tweets\ntraindoc = pd.DataFrame(docs)\ntraindoc.index = df.index\ntraindoc = traindoc.join([df.LABEL , df.TARGET])\ntraindoc = traindoc[traindoc.LABEL == 'train']\n\nreal = traindoc[traindoc.TARGET == 1]\nreal = real.drop(columns = ['LABEL','TARGET'])\nfake = traindoc[traindoc.TARGET == 0]\nfake = fake.drop(columns = ['LABEL','TARGET'])\n#print(len(fake) + len(real)) #Also makes sure there aren't any targets labled other than 1 or 0\n\n\n#This will tally the number of mispelled words and the total number of words for disaster tweets\nspellcheck = SpellChecker()\nmispelledreal = 0\ntotalreal = 0\nfor tweet in real.iloc[:,0]:\n    buff = re.split('\\W+',tweet)\n    mispelledreal += len(spellcheck.unknown(buff))\n    totalreal += len(buff)\n\nprint(mispelledreal/totalreal)\n\n#This will tally the number of mispelled words and the total number of words for fake disaster tweets\nmispelledfake = 0\ntotalfake = 0\nfor tweet in fake.iloc[:,0]:\n    buff = re.split('\\W+',tweet)\n    mispelledfake += len(spellcheck.unknown(buff))\n    totalfake += len(buff)\n    \nprint(mispelledfake/totalfake)\n    \nprint((mispelledreal/totalreal)/(mispelledfake/totalfake))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Turns out there's only a very small difference between spelling mistakes.  However, this is likely also taking into account words that the spellchecker just dosn't recognize (like URLs and places)\n\nWhat about the total number of words in each of the tweets?"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(totalreal/len(real))\nprint(totalfake/len(fake))\nprint((totalreal/len(real))/(totalfake/len(fake)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above, we can see that real disaster tweets are about 1 word longer on average than are the fake ones.\n\nLet's see if the length of the words themselves are longer."},{"metadata":{"trusted":true},"cell_type":"code","source":"fakechars = 0\nfor tweet in fake.iloc[:,0]:\n    fakechars += len(tweet)\n    \nrealchars = 0\nfor tweet in real.iloc[:,0]:\n    realchars += len(tweet)\n    \nprint(realchars/totalreal)\nprint(fakechars/totalfake)  \nprint((realchars/totalreal)/(fakechars/totalfake))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The length of real words is about 0.5 chars longer on average than fake ones (including whitespace).  May not be the best representation of word length, but there was a significant difference.\n\nFinally, let's just look at the length of the tweets themselves (in chars)"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(realchars/len(real))\nprint(fakechars/len(fake))\nprint((realchars/len(real))/(fakechars/len(fake)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here there was also a noticable difference.  Later, we'll take some of these observations and make features of them."},{"metadata":{},"cell_type":"markdown","source":"## Data Vectorization\n\nNow that the text has been prepared, we'll vectorize it so it can be used in a model.  \n\nTF-IDF will determine the relative frequency of each word in a tweet compared to the frequency of that word amongst all tweets.  It will offer a bit more context than just a vector that only describes the presence of words (Bag of Words)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Makes the TF-IDF vectors and puts them together into a dataframe\n#Need to limit the number of features to reduce the space it takes up\nvectorizer = TfidfVectorizer(max_features = 1000)\nX = vectorizer.fit_transform(docs)\nterms = vectorizer.get_feature_names()\ndense = X.todense()\ndenselist = dense.tolist()\n\ntfidf = pd.DataFrame(denselist, columns = terms, index = df.index)\n\nprint(tfidf.shape)\nprint(tfidf.head())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Other Features\nIncluded in the datasets are associated keywords, and locations.  We will want to take these into consideration.  We'll keep track of what specific keywords exist or not (1 or 0 for each possible keyword), and whether or not a location is given (dosn't matter where, just if one is specified; there were alot of locations)."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Adds columns for the different Keywords, and whether or not they occured.  Also adds the string 'KEY' to the columns to differentiate between TF-IDF words.\nkeywords = pd.get_dummies(df.KEYWORD)\nkeywords.columns = [str(x) + 'KEY' for x in keywords.columns]\nkeywords = keywords.set_index(df.index)\nprint(keywords.head())\n#Adds a simple column for location.  1 means a location was specified, 0 means it wasn't.\n\nlocation = []\nfor loca in df.LOCATION:\n    if pd.isnull(loca):\n        location.append(0)\n    else:\n        location.append(1)\nlocation = pd.DataFrame(location, columns = [\"LOCATION\"])\nlocation = location.set_index(df.index)\nprint(location.head())\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, remember how we looked at word spelling and length averages?  Let's now make some features based off those observations"},{"metadata":{"trusted":true},"cell_type":"code","source":"#This makes a dataset for the wordcounts in each tweet\nwordcount = []\nfor tweet in docs:\n    wordcount.append(len(re.split('\\W+',tweet)))\n\nwords = pd.DataFrame(wordcount, columns = [\"WORDS\"])\nwords = words.set_index(df.index)\n    \n    \n#This makes a datacet of the number of characters in each tweet\ncharcount = []\n\nfor tweet in docs:\n    charcount.append(len(tweet))\n    \nchars = pd.DataFrame(charcount, columns = [\"CHARS\"])\nchars = chars.set_index(df.index)\n\n#This makes a dataset of the number of characters per word\ncharperword = list(map(truediv, charcount, wordcount))\n\ncpw = pd.DataFrame(charperword, columns = [\"CPW\"])\ncpw = cpw.set_index(df.index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Final preparation\nAll that's left to do now is to join all the processed data together (TF-IDF, Keywords, and Location, word/char counts), and split it back up into training and testing sets."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Combines the previous datasets into combined large set; also adds back in the lables.  \n#Note: everything has been index consistantly on the ID of each tweet throughout, so it all ligns up here\ncombined = tfidf.join(keywords)\ncombined = combined.join(location)\ncombined = combined.join(df.LABEL)\ncombined = combined.join(words)\ncombined = combined.join(chars)\ncombined = combined.join(cpw)\n\n#Splits the dataset back into training and testing sets, and removes the lables\ntrain_prepped = combined[combined['LABEL'] == 'train']\ntrain_prepped = train_prepped.drop(columns=['LABEL'])\ntest_prepped = combined[combined['LABEL'] == 'test']\ntest_prepped = test_prepped.drop(columns=['LABEL'])\nprint(test_prepped.head())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Building and Tuning\nHe're going to employ a random forest model with 100 estimators\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"RFmodel = RandomForestClassifier(n_estimators=100, \n                                bootstrap = True,\n                                max_features = 'sqrt')\n\nRFmodel.fit(train_prepped, train_target)\npredictions = RFmodel.predict(test_prepped)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = pd.DataFrame({'id': test_prepped.index, 'target': predictions})\noutput.to_csv('predictions.csv', index=False)\nprint(\"Your submission was successfully saved!\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Results and Discussion\n\nThis model yielded predictions on the testing set that were about 78% correct.  For comparison, guessing that all of the tweets were fake would have been 57% correct.  This suggests that our model is in fact making informed predictions that are somewhat effective.  \n\nThis model could be improved by tinkering with the model parameters, or perhaps by creating more features and filtering out more noise from the text data."}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}